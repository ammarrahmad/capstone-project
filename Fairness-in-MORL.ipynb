{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVB7IitjSIoN",
        "outputId": "9cab16cb-e7fa-4203-d69e-06bd804b8385"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mo-gymnasium in /usr/local/lib/python3.12/dist-packages (1.3.1)\n",
            "Requirement already satisfied: morl-baselines in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.22.3)\n",
            "Requirement already satisfied: gymnasium>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (1.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (2.6.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.12/dist-packages (from mo-gymnasium) (1.16.3)\n",
            "Requirement already satisfied: torch>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (2.8.0+cu126)\n",
            "Requirement already satisfied: pymoo>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (0.6.1.5)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (2.37.2)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (1.0.3)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (0.13.2)\n",
            "Requirement already satisfied: cvxpy in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (1.6.7)\n",
            "Requirement already satisfied: fire in /usr/local/lib/python3.12/dist-packages (from morl-baselines) (0.7.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.11.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.32.4)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.44.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.8 in /usr/local/lib/python3.12/dist-packages (from wandb) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0->mo-gymnasium) (3.1.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0->mo-gymnasium) (0.0.4)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (3.10.0)\n",
            "Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (1.8.0)\n",
            "Requirement already satisfied: cma>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (4.4.0)\n",
            "Requirement already satisfied: alive-progress in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (3.3.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (0.3.8)\n",
            "Requirement already satisfied: Deprecated in /usr/local/lib/python3.12/dist-packages (from pymoo>=0.6.0->morl-baselines) (1.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.0.0->wandb) (2025.10.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.12.0->morl-baselines) (3.4.0)\n",
            "Requirement already satisfied: osqp>=0.6.2 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (1.0.5)\n",
            "Requirement already satisfied: clarabel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (0.11.1)\n",
            "Requirement already satisfied: scs>=3.2.4.post1 in /usr/local/lib/python3.12/dist-packages (from cvxpy->morl-baselines) (3.2.9)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from fire->morl-baselines) (3.2.0)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio->morl-baselines) (11.3.0)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (4.67.1)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (0.1.12)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy->morl-baselines) (0.6.0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn->morl-baselines) (2.2.2)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.12/dist-packages (from clarabel>=0.5.0->cvxpy->morl-baselines) (2.0.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3->pymoo>=0.6.0->morl-baselines) (2.9.0.post0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from osqp>=0.6.2->cvxpy->morl-baselines) (1.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->morl-baselines) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn->morl-baselines) (2025.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.12.0->morl-baselines) (1.3.0)\n",
            "Requirement already satisfied: about-time==4.2.1 in /usr/local/lib/python3.12/dist-packages (from alive-progress->pymoo>=0.6.0->morl-baselines) (4.2.1)\n",
            "Requirement already satisfied: graphemeu==0.7.2 in /usr/local/lib/python3.12/dist-packages (from alive-progress->pymoo>=0.6.0->morl-baselines) (0.7.2)\n",
            "Requirement already satisfied: wrapt<3,>=1.10 in /usr/local/lib/python3.12/dist-packages (from Deprecated->pymoo>=0.6.0->morl-baselines) (2.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.12.0->morl-baselines) (3.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3->pymoo>=0.6.0->morl-baselines) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi->clarabel>=0.5.0->cvxpy->morl-baselines) (2.23)\n"
          ]
        }
      ],
      "source": [
        "!pip install mo-gymnasium morl-baselines wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fa9EZcZtZma0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ygo7CtO4Plf"
      },
      "outputs": [],
      "source": [
        "from morl_baselines.common.evaluation import eval_mo\n",
        "import mo_gymnasium as mo_gym\n",
        "from morl_baselines.multi_policy.envelope.envelope import Envelope\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4haHTlkbkic"
      },
      "outputs": [],
      "source": [
        "# from morl_baselines.common.evaluation import eval_mo\n",
        "# import mo_gymnasium as mo_gym\n",
        "# from morl_baselines.multi_policy.envelope.envelope import Envelope\n",
        "# import numpy as np\n",
        "\n",
        "# env = mo_gym.make(\"resource-gathering-v0\")\n",
        "# eval_env = mo_gym.make(\"resource-gathering-v0\")\n",
        "\n",
        "# agent = Envelope(\n",
        "#     env,\n",
        "#     log=True,\n",
        "# )\n",
        "\n",
        "# agent.train(\n",
        "#     total_timesteps=1000,\n",
        "#     eval_env=eval_env,\n",
        "#     ref_point=np.array([0.0, 0.0, -200.0]),\n",
        "#     eval_freq=100,\n",
        "# )\n",
        "\n",
        "# scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=np.array([0.5, 0.4, 0.1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nNJtprSbxet"
      },
      "outputs": [],
      "source": [
        "\"\"\"FMDQ (Fair Multi-Objective DQN) implementation.\"\"\"\n",
        "\n",
        "import os\n",
        "from typing import List, Optional, Union\n",
        "from typing_extensions import override\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "\n",
        "from morl_baselines.common.buffer import ReplayBuffer\n",
        "from morl_baselines.common.evaluation import (\n",
        "    log_all_multi_policy_metrics,\n",
        "    log_episode_info,\n",
        ")\n",
        "from morl_baselines.common.morl_algorithm import MOAgent, MOPolicy\n",
        "from morl_baselines.common.networks import (\n",
        "    NatureCNN,\n",
        "    get_grad_norm,\n",
        "    layer_init,\n",
        "    mlp,\n",
        "    polyak_update,\n",
        ")\n",
        "from morl_baselines.common.prioritized_buffer import PrioritizedReplayBuffer\n",
        "from morl_baselines.common.utils import linearly_decaying_value\n",
        "from morl_baselines.common.weights import equally_spaced_weights, random_weights\n",
        "\n",
        "\n",
        "def np_ggf_scalarization(v: np.ndarray, w: np.ndarray) -> float:\n",
        "    \"\"\"Numpy GGF scalarization for logging.\"\"\"\n",
        "    # Ensure w is 1d\n",
        "    if w.ndim > 1:\n",
        "        w = w.flatten()\n",
        "    # Sort v ascending, w descending\n",
        "    v_sorted = np.sort(v)\n",
        "    w_sorted = np.sort(w)[::-1]\n",
        "    return np.dot(v_sorted, w_sorted)\n",
        "\n",
        "\n",
        "class VectorQNet(nn.Module):\n",
        "    \"\"\"Multi-objective Q-Network (outputs a Q-vector for each action).\"\"\"\n",
        "\n",
        "    def __init__(self, obs_shape, action_dim, rew_dim, net_arch):\n",
        "        \"\"\"Initialize the Q network.\n",
        "\n",
        "        Args:\n",
        "            obs_shape: shape of the observation\n",
        "            action_dim: number of actions\n",
        "            rew_dim: number of objectives\n",
        "            net_arch: network architecture (number of units per layer)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.obs_shape = obs_shape\n",
        "        self.action_dim = action_dim\n",
        "        self.rew_dim = rew_dim\n",
        "        if len(obs_shape) == 1:\n",
        "            self.feature_extractor = None\n",
        "            input_dim = obs_shape[0]\n",
        "        elif len(obs_shape) > 1:  # Image observation\n",
        "            self.feature_extractor = NatureCNN(self.obs_shape, features_dim=512)\n",
        "            input_dim = self.feature_extractor.features_dim\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid observation shape: {obs_shape}\")\n",
        "\n",
        "        # |S| -> ... -> |A| * |R|\n",
        "        self.net = mlp(input_dim, action_dim * rew_dim, net_arch)\n",
        "        self.apply(layer_init)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"Predict Q-vectors for all actions.\n",
        "\n",
        "        Args:\n",
        "            obs: current observation\n",
        "\n",
        "        Returns:\n",
        "            Q-vectors for all actions, shape (batch_size, action_dim, rew_dim)\n",
        "        \"\"\"\n",
        "        if self.feature_extractor is not None:\n",
        "            features = self.feature_extractor(obs)\n",
        "        else:\n",
        "            # Handle non-batched observations for MLP\n",
        "            if obs.dim() == 1:\n",
        "                obs = obs.unsqueeze(0)\n",
        "            features = obs\n",
        "\n",
        "        q_values = self.net(features)\n",
        "        return q_values.view(-1, self.action_dim, self.rew_dim)  # Batch size X Actions X Rewards\n",
        "\n",
        "\n",
        "class FMDQ(MOPolicy, MOAgent):\n",
        "    \"\"\"FMDQ Algorithm.\n",
        "\n",
        "    This algorithm learns a vector-valued Q-function and uses the Generalized Gini Welfare (GGF)\n",
        "    function for action selection and target updates to learn a set of \"fair\" Pareto-optimal policies.\n",
        "    It is based on the paper \"Learning Fair Pareto Optimal Policies in Multi-Objective Reinforcement Learning\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env,\n",
        "        learning_rate: float = 3e-4,\n",
        "        initial_epsilon: float = 1.0,\n",
        "        final_epsilon: float = 0.05,\n",
        "        epsilon_decay_steps: int = 50000,\n",
        "        tau: float = 1.0,\n",
        "        target_net_update_freq: int = 200,  # ignored if tau != 1.0\n",
        "        buffer_size: int = int(1e6),\n",
        "        net_arch: List = [256, 256],\n",
        "        batch_size: int = 256,\n",
        "        learning_starts: int = 100,\n",
        "        gradient_updates: int = 1,\n",
        "        gamma: float = 0.99,\n",
        "        max_grad_norm: Optional[float] = 1.0,\n",
        "        per: bool = True,\n",
        "        per_alpha: float = 0.6,\n",
        "        project_name: str = \"MORL-Baselines\",\n",
        "        experiment_name: str = \"FMDQ\",\n",
        "        wandb_entity: Optional[str] = None,\n",
        "        log: bool = True,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        group: Optional[str] = None,\n",
        "    ):\n",
        "        \"\"\"FMDQ algorithm.\n",
        "\n",
        "        Args:\n",
        "            env: The environment to learn from.\n",
        "            learning_rate: The learning rate (alpha).\n",
        "            initial_epsilon: The initial epsilon value for epsilon-greedy exploration.\n",
        "            final_epsilon: The final epsilon value for epsilon-greedy exploration.\n",
        "            epsilon_decay_steps: The number of steps to decay epsilon over.\n",
        "            tau: The soft update coefficient (keep in [0, 1]).\n",
        "            target_net_update_freq: The frequency with which the target network is updated.\n",
        "            buffer_size: The size of the replay buffer.\n",
        "            net_arch: The size of the hidden layers of the value net.\n",
        "            batch_size: The size of the batch to sample from the replay buffer.\n",
        "            learning_starts: The number of steps before learning starts.\n",
        "            gradient_updates: The number of gradient updates per step.\n",
        "            gamma: The discount factor (gamma).\n",
        "            max_grad_norm: The maximum norm for the gradient clipping. If None, no gradient clipping is applied.\n",
        "            per: Whether to use prioritized experience replay.\n",
        "            per_alpha: The alpha parameter for prioritized experience replay.\n",
        "            project_name: The name of the project, for wandb logging.\n",
        "            experiment_name: The name of the experiment, for wandb logging.\n",
        "            wandb_entity: The entity of the project, for wandb logging.\n",
        "            log: Whether to log to wandb.\n",
        "            seed: The seed for the random number generator.\n",
        "            device: The device to use for training.\n",
        "            group: The wandb group to use for logging.\n",
        "        \"\"\"\n",
        "        MOAgent.__init__(self, env, device=device, seed=seed)\n",
        "        MOPolicy.__init__(self, device)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initial_epsilon = initial_epsilon\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay_steps = epsilon_decay_steps\n",
        "        self.final_epsilon = final_epsilon\n",
        "        self.tau = tau\n",
        "        self.target_net_update_freq = target_net_update_freq\n",
        "        self.gamma = gamma\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.buffer_size = buffer_size\n",
        "        self.net_arch = net_arch\n",
        "        self.learning_starts = learning_starts\n",
        "        self.batch_size = batch_size\n",
        "        self.per = per\n",
        "        self.per_alpha = per_alpha\n",
        "        self.gradient_updates = gradient_updates\n",
        "\n",
        "        self.q_net = VectorQNet(self.observation_shape, self.action_dim, self.reward_dim, net_arch=net_arch).to(self.device)\n",
        "        self.target_q_net = VectorQNet(self.observation_shape, self.action_dim, self.reward_dim, net_arch=net_arch).to(\n",
        "            self.device\n",
        "        )\n",
        "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
        "        for param in self.target_q_net.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.q_optim = optim.Adam(self.q_net.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        if self.per:\n",
        "            self.replay_buffer = PrioritizedReplayBuffer(\n",
        "                self.observation_shape,\n",
        "                1,\n",
        "                rew_dim=self.reward_dim,\n",
        "                max_size=buffer_size,\n",
        "                action_dtype=np.uint8,\n",
        "            )\n",
        "        else:\n",
        "            self.replay_buffer = ReplayBuffer(\n",
        "                self.observation_shape,\n",
        "                1,\n",
        "                rew_dim=self.reward_dim,\n",
        "                max_size=buffer_size,\n",
        "                action_dtype=np.uint8,\n",
        "            )\n",
        "\n",
        "        self.log = log\n",
        "        if log:\n",
        "            self.setup_wandb(project_name, experiment_name, wandb_entity, group)\n",
        "\n",
        "    @override\n",
        "    def get_config(self):\n",
        "        return {\n",
        "            \"env_id\": self.env.unwrapped.spec.id,\n",
        "            \"learning_rate\": self.learning_rate,\n",
        "            \"initial_epsilon\": self.initial_epsilon,\n",
        "            \"epsilon_decay_steps\": self.epsilon_decay_steps,\n",
        "            \"batch_size\": self.batch_size,\n",
        "            \"tau\": self.tau,\n",
        "            \"clip_grand_norm\": self.max_grad_norm,\n",
        "            \"target_net_update_freq\": self.target_net_update_freq,\n",
        "            \"gamma\": self.gamma,\n",
        "            \"net_arch\": self.net_arch,\n",
        "            \"per\": self.per,\n",
        "            \"gradient_updates\": self.gradient_updates,\n",
        "            \"buffer_size\": self.buffer_size,\n",
        "            \"learning_starts\": self.learning_starts,\n",
        "            \"seed\": self.seed,\n",
        "        }\n",
        "\n",
        "    def save(self, save_replay_buffer: bool = True, save_dir: str = \"weights/\", filename: Optional[str] = None):\n",
        "        if not os.path.isdir(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "        saved_params = {}\n",
        "        saved_params[\"q_net_state_dict\"] = self.q_net.state_dict()\n",
        "\n",
        "        saved_params[\"q_net_optimizer_state_dict\"] = self.q_optim.state_dict()\n",
        "        if save_replay_buffer:\n",
        "            saved_params[\"replay_buffer\"] = self.replay_buffer\n",
        "        filename = self.experiment_name if filename is None else filename\n",
        "        th.save(saved_params, save_dir + \"/\" + filename + \".tar\")\n",
        "\n",
        "    def load(self, path: str, load_replay_buffer: bool = True):\n",
        "        params = th.load(path, weights_only=False)\n",
        "        self.q_net.load_state_dict(params[\"q_net_state_dict\"])\n",
        "        self.target_q_net.load_state_dict(params[\"q_net_state_dict\"])\n",
        "        self.q_optim.load_state_dict(params[\"q_net_optimizer_state_dict\"])\n",
        "        if load_replay_buffer and \"replay_buffer\" in params:\n",
        "            self.replay_buffer = params[\"replay_buffer\"]\n",
        "\n",
        "    def __sample_batch_experiences(self):\n",
        "        return self.replay_buffer.sample(self.batch_size, to_tensor=True, device=self.device)\n",
        "\n",
        "    def ggf_scalarization(self, q_vectors: th.Tensor, w: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"Applies GGF scalarization.\n",
        "\n",
        "        Args:\n",
        "            q_vectors: (batch_size, num_actions, rew_dim)\n",
        "            w: (batch_size, 1, rew_dim) or (batch_size, rew_dim)\n",
        "\n",
        "        Returns:\n",
        "            Scalarized Q-values (batch_size, num_actions)\n",
        "        \"\"\"\n",
        "        if w.dim() == 2:  # (B, R)\n",
        "            w = w.unsqueeze(1)  # (B, 1, R)\n",
        "        # Sort Q-vectors ascending along the reward dimension\n",
        "        q_sorted, _ = th.sort(q_vectors, dim=-1, descending=False)\n",
        "        # Sort weights descending along the reward dimension\n",
        "        w_sorted, _ = th.sort(w, dim=-1, descending=True)\n",
        "        # GGF is the dot product of sorted vectors\n",
        "        # w_sorted broadcasts across the action dimension\n",
        "        return th.sum(q_sorted * w_sorted, dim=-1)\n",
        "\n",
        "    def ggf_scalarization_vec(self, v: th.Tensor, w: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"Applies GGF scalarization to 1D vectors.\n",
        "\n",
        "        Args:\n",
        "            v: (batch_size, rew_dim)\n",
        "            w: (batch_size, rew_dim)\n",
        "\n",
        "        Returns:\n",
        "            Scalarized values (batch_size,)\n",
        "        \"\"\"\n",
        "        v_sorted, _ = th.sort(v, dim=-1, descending=False)\n",
        "        w_sorted, _ = th.sort(w, dim=-1, descending=True)\n",
        "        return th.sum(v_sorted * w_sorted, dim=-1)\n",
        "\n",
        "    @override\n",
        "    def update(self):\n",
        "        critic_losses = []\n",
        "        for g in range(self.gradient_updates):\n",
        "            if self.per:\n",
        "                (\n",
        "                    b_obs,\n",
        "                    b_actions,\n",
        "                    b_rewards,\n",
        "                    b_next_obs,\n",
        "                    b_dones,\n",
        "                    b_inds,\n",
        "                ) = self.__sample_batch_experiences()\n",
        "            else:\n",
        "                (\n",
        "                    b_obs,\n",
        "                    b_actions,\n",
        "                    b_rewards,\n",
        "                    b_next_obs,\n",
        "                    b_dones,\n",
        "                ) = self.__sample_batch_experiences()\n",
        "\n",
        "            # Sample a weight vector for each transition in the batch\n",
        "            b_w = (\n",
        "                th.tensor(random_weights(dim=self.reward_dim, n=self.batch_size, dist=\"gaussian\", rng=self.np_random))\n",
        "                .float()\n",
        "                .to(self.device)\n",
        "            )\n",
        "\n",
        "            with th.no_grad():\n",
        "                target_q_vecs = self.fmdq_target(b_next_obs, b_w)\n",
        "                target_q = b_rewards + (1 - b_dones) * self.gamma * target_q_vecs\n",
        "\n",
        "            q_values = self.q_net(b_obs)\n",
        "            q_value = q_values.gather(\n",
        "                1,\n",
        "                b_actions.long().reshape(-1, 1, 1).expand(q_values.size(0), 1, q_values.size(2)),\n",
        "            )\n",
        "            q_value = q_value.squeeze(1)  # (batch_size, rew_dim)\n",
        "\n",
        "            critic_loss = F.mse_loss(q_value, target_q)  # Vector-valued MSE loss\n",
        "\n",
        "            self.q_optim.zero_grad()\n",
        "            critic_loss.backward()\n",
        "\n",
        "            if self.log and self.global_step % 100 == 0:\n",
        "                wandb.log(\n",
        "                    {\n",
        "                        \"losses/grad_norm\": get_grad_norm(self.q_net.parameters()).item(),\n",
        "                        \"global_step\": self.global_step,\n",
        "                    },\n",
        "                )\n",
        "            if self.max_grad_norm is not None:\n",
        "                th.nn.utils.clip_grad_norm_(self.q_net.parameters(), self.max_grad_norm)\n",
        "            self.q_optim.step()\n",
        "            critic_losses.append(critic_loss.item())\n",
        "\n",
        "            if self.per:\n",
        "                # Calculate TD-error using GGF scalarization for priority\n",
        "                td_err = (q_value - target_q).detach()\n",
        "                priority = self.ggf_scalarization_vec(td_err, b_w).abs()\n",
        "                priority = priority.cpu().numpy().flatten()\n",
        "                priority = (priority + self.replay_buffer.min_priority) ** self.per_alpha\n",
        "                self.replay_buffer.update_priorities(b_inds, priority)\n",
        "\n",
        "        if self.tau != 1 or self.global_step % self.target_net_update_freq == 0:\n",
        "            polyak_update(self.q_net.parameters(), self.target_q_net.parameters(), self.tau)\n",
        "\n",
        "        if self.epsilon_decay_steps is not None:\n",
        "            self.epsilon = linearly_decaying_value(\n",
        "                self.initial_epsilon,\n",
        "                self.epsilon_decay_steps,\n",
        "                self.global_step,\n",
        "                self.learning_starts,\n",
        "                self.final_epsilon,\n",
        "            )\n",
        "\n",
        "        if self.log and self.global_step % 100 == 0:\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"losses/critic_loss\": np.mean(critic_losses),\n",
        "                    \"metrics/epsilon\": self.epsilon,\n",
        "                    \"global_step\": self.global_step,\n",
        "                },\n",
        "            )\n",
        "            if self.per:\n",
        "                wandb.log({\"metrics/mean_priority\": np.mean(priority)})\n",
        "\n",
        "    @override\n",
        "    def eval(self, obs: np.ndarray, w: np.ndarray) -> int:\n",
        "        obs_tensor = th.as_tensor(obs).float().to(self.device)\n",
        "        w_tensor = th.as_tensor(w).float().to(self.device)\n",
        "        return self.max_action(obs_tensor, w_tensor)\n",
        "\n",
        "    def act(self, obs: th.Tensor, w: th.Tensor) -> int:\n",
        "        \"\"\"Epsilon-greedily select an action given an observation and weight.\"\"\"\n",
        "        if self.np_random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return self.max_action(obs, w)\n",
        "\n",
        "    @th.no_grad()\n",
        "    def max_action(self, obs: th.Tensor, w: th.Tensor) -> int:\n",
        "        \"\"\"Select the action with the highest GGF Q-value.\"\"\"\n",
        "        # Add batch dim if missing\n",
        "        if w.dim() == 1:\n",
        "            w = w.unsqueeze(0)\n",
        "        if obs.dim() < len(self.observation_shape) + 1:\n",
        "            obs = obs.unsqueeze(0)\n",
        "\n",
        "        q_values = self.q_net(obs)  # (1, num_actions, rew_dim)\n",
        "        scalarized_q_values = self.ggf_scalarization(q_values, w.unsqueeze(1))  # (1, num_actions)\n",
        "        max_act = th.argmax(scalarized_q_values, dim=1)\n",
        "        return max_act.detach().item()\n",
        "\n",
        "    @th.no_grad()\n",
        "    def fmdq_target(self, obs: th.Tensor, w: th.Tensor) -> th.Tensor:\n",
        "        \"\"\"FMDQ target (DDQN target with GGF for action selection).\n",
        "\n",
        "        Args:\n",
        "            obs: Next observation (batch_size, obs_shape)\n",
        "            w: Weight vector (batch_size, rew_dim)\n",
        "\n",
        "        Returns:\n",
        "            The target Q-vector (batch_size, rew_dim)\n",
        "        \"\"\"\n",
        "        # 1. Select best action a' using main Q-net and GGF\n",
        "        q_values = self.q_net(obs)  # (B, A, R)\n",
        "        scalarized_q_values = self.ggf_scalarization(q_values, w.unsqueeze(1))  # (B, A)\n",
        "        max_acts = th.argmax(scalarized_q_values, dim=1)  # (B,)\n",
        "\n",
        "        # 2. Get Q-vector for a' from target Q-net\n",
        "        q_values_target = self.target_q_net(obs)  # (B, A, R)\n",
        "\n",
        "        # Gather the Q-vector corresponding to max_acts\n",
        "        q_values_target = q_values_target.gather(\n",
        "            1,\n",
        "            max_acts.long().reshape(-1, 1, 1).expand(-1, 1, self.reward_dim),\n",
        "        )\n",
        "        q_values_target = q_values_target.squeeze(1)  # (B, R)\n",
        "        return q_values_target\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        total_timesteps: int,\n",
        "        eval_env: Optional[gym.Env] = None,\n",
        "        ref_point: Optional[np.ndarray] = None,\n",
        "        known_pareto_front: Optional[List[np.ndarray]] = None,\n",
        "        weight: Optional[np.ndarray] = None,\n",
        "        total_episodes: Optional[int] = None,\n",
        "        reset_num_timesteps: bool = True,\n",
        "        eval_freq: int = 10000,\n",
        "        num_eval_weights_for_front: int = 100,\n",
        "        num_eval_episodes_for_front: int = 5,\n",
        "        num_eval_weights_for_eval: int = 50,\n",
        "        reset_learning_starts: bool = False,\n",
        "        verbose: bool = False,\n",
        "    ):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        if eval_env is not None:\n",
        "            assert ref_point is not None, \"Reference point must be provided for the hypervolume computation.\"\n",
        "        if self.log:\n",
        "            self.register_additional_config(\n",
        "                {\n",
        "                    \"total_timesteps\": total_timesteps,\n",
        "                    \"ref_point\": ref_point.tolist() if ref_point is not None else None,\n",
        "                    \"known_front\": known_pareto_front,\n",
        "                    \"weight\": weight.tolist() if weight is not None else None,\n",
        "                    \"total_episodes\": total_episodes,\n",
        "                    \"reset_num_timesteps\": reset_num_timesteps,\n",
        "                    \"eval_freq\": eval_freq,\n",
        "                    \"num_eval_weights_for_front\": num_eval_weights_for_front,\n",
        "                    \"num_eval_episodes_for_front\": num_eval_episodes_for_front,\n",
        "                    \"num_eval_weights_for_eval\": num_eval_weights_for_eval,\n",
        "                    \"reset_learning_starts\": reset_learning_starts,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        self.global_step = 0 if reset_num_timesteps else self.global_step\n",
        "        self.num_episodes = 0 if reset_num_timesteps else self.num_episodes\n",
        "        if reset_learning_starts:\n",
        "            self.learning_starts = self.global_step\n",
        "\n",
        "        num_episodes = 0\n",
        "        eval_weights = equally_spaced_weights(self.reward_dim, n=num_eval_weights_for_front)\n",
        "        obs, _ = self.env.reset()\n",
        "\n",
        "        w = weight if weight is not None else random_weights(self.reward_dim, 1, dist=\"gaussian\", rng=self.np_random)\n",
        "        tensor_w = th.tensor(w).float().to(self.device)\n",
        "\n",
        "        for _ in range(1, total_timesteps + 1):\n",
        "            if total_episodes is not None and num_episodes == total_episodes:\n",
        "                break\n",
        "\n",
        "            if self.global_step < self.learning_starts:\n",
        "                action = self.env.action_space.sample()\n",
        "            else:\n",
        "                action = self.act(th.as_tensor(obs).float().to(self.device), tensor_w)\n",
        "\n",
        "            next_obs, vec_reward, terminated, truncated, info = self.env.step(action)\n",
        "            self.global_step += 1\n",
        "\n",
        "            self.replay_buffer.add(obs, action, vec_reward, next_obs, terminated)\n",
        "            if self.global_step >= self.learning_starts:\n",
        "                self.update()\n",
        "\n",
        "            if eval_env is not None and self.log and self.global_step % eval_freq == 0:\n",
        "                current_front = [\n",
        "                    self.policy_eval(eval_env, weights=ew, num_episodes=num_eval_episodes_for_front, log=self.log)[3]\n",
        "                    for ew in eval_weights\n",
        "                ]\n",
        "                log_all_multi_policy_metrics(\n",
        "                    current_front=current_front,\n",
        "                    hv_ref_point=ref_point,\n",
        "                    reward_dim=self.reward_dim,\n",
        "                    global_step=self.global_step,\n",
        "                    n_sample_weights=num_eval_weights_for_eval,\n",
        "                    ref_front=known_pareto_front,\n",
        "                )\n",
        "\n",
        "            if terminated or truncated:\n",
        "                obs, _ = self.env.reset()\n",
        "                num_episodes += 1\n",
        "                self.num_episodes += 1\n",
        "\n",
        "                if self.log and \"episode\" in info.keys():\n",
        "                    # Log with GGF scalarization instead of dot product\n",
        "                    log_episode_info(info[\"episode\"], np_ggf_scalarization, w, self.global_step, verbose=verbose)\n",
        "\n",
        "                if weight is None:\n",
        "                    w = random_weights(self.reward_dim, 1, dist=\"gaussian\", rng=self.np_random)\n",
        "                    tensor_w = th.tensor(w).float().to(self.device)\n",
        "\n",
        "            else:\n",
        "                obs = next_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "Py_ELMK__zSQ",
        "outputId": "51767bbe-f28e-4d7b-a159-5356121e3b3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m2201ai04_ammar\u001b[0m (\u001b[33m2201ai04_ammar-indian-institute-of-technology-patna\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.22.3"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20251113_220215-2r306h9u</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1/runs/2r306h9u' target=\"_blank\">minecart-v0__FMDQ__None__1763071328</a></strong> to <a href='https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1' target=\"_blank\">https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1/runs/2r306h9u' target=\"_blank\">https://wandb.ai/2201ai04_ammar-indian-institute-of-technology-patna/Exp-1/runs/2r306h9u</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# from morl_baselines.common.evaluation import eval_mo\n",
        "# import mo_gymnasium as mo_gym\n",
        "\n",
        "# env = mo_gym.make(\"resource-gathering-v0\")\n",
        "# eval_env = mo_gym.make(\"resource-gathering-v0\")\n",
        "\n",
        "# agent = FMDQ(\n",
        "#     env,\n",
        "#     log=True,\n",
        "#     project_name = \"Exp-1\"\n",
        "# )\n",
        "\n",
        "# agent.train(\n",
        "#     total_timesteps=5000,\n",
        "#     eval_env=eval_env,\n",
        "#     ref_point=np.array([0.0, 0.0, -200.0]),\n",
        "#     eval_freq=10,\n",
        "# )\n",
        "\n",
        "# scalar_return, scalarized_disc_return, vec_ret, vec_disc_ret = eval_mo(agent, env=eval_env, w=np.array([0.5, 0.4, 0.1]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQUD24mcNf6Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pymoo.indicators.hv import HV\n",
        "import mo_gymnasium as mo_gym\n",
        "from morl_baselines.common.evaluation import eval_mo\n",
        "from morl_baselines.multi_policy.envelope.envelope import Envelope\n",
        "\n",
        "def compute_metrics(name, returns):\n",
        "    print(f\"\\n===== {name} RESULTS =====\")\n",
        "\n",
        "    # Hypervolume\n",
        "    ref = np.array([0.0, 0.0])\n",
        "    hv = HV(ref_point=ref)(returns)\n",
        "    print(\"Hypervolume:\", hv)\n",
        "\n",
        "    # Cardinality\n",
        "    cd = len(returns)\n",
        "    print(\"Cardinality:\", cd)\n",
        "\n",
        "    # Totals, max, min\n",
        "    total_resources = returns.sum(axis=1)\n",
        "    print(\"Avg Total:\", total_resources.mean())\n",
        "    print(\"Avg Max:\", returns.max(axis=1).mean())\n",
        "    print(\"Avg Min:\", returns.min(axis=1).mean())\n",
        "\n",
        "    # CV\n",
        "    cv_per_obj = returns.std(axis=0) / returns.mean(axis=0)\n",
        "    cv_total = total_resources.std() / total_resources.mean()\n",
        "    print(\"CV per objective:\", cv_per_obj)\n",
        "    print(\"CV total:\", cv_total)\n",
        "\n",
        "    # GGF\n",
        "    def ggf(v):\n",
        "        v = np.array(v)\n",
        "        d = len(v)\n",
        "        v_sorted = np.sort(v)\n",
        "        lambdas = np.array([(d - i) / d for i in range(1, d+1)])\n",
        "        return np.sum(lambdas * v_sorted)\n",
        "\n",
        "    ggf_scores = np.array([ggf(v) for v in returns])\n",
        "    print(\"GGF mean:\", ggf_scores.mean())\n",
        "    print(\"GGF min:\", ggf_scores.min())\n",
        "    print(\"GGF max:\", ggf_scores.max())\n",
        "\n",
        "    return {\n",
        "        \"hv\": hv,\n",
        "        \"cd\": cd,\n",
        "        \"cv\": cv_total,\n",
        "        \"ggf_mean\": ggf_scores.mean()\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_front(returns, title):\n",
        "    plt.scatter(returns[:,0], returns[:,1])\n",
        "    plt.xlabel(\"Resource 1\")\n",
        "    plt.ylabel(\"Resource 2\")\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = mo_gym.make(\"resource-gathering-v0\")\n",
        "eval_env = mo_gym.make(\"resource-gathering-v0\")\n",
        "\n",
        "envelope_agent = Envelope(\n",
        "    env,\n",
        "    log=True,\n",
        ")\n",
        "\n",
        "envelope_agent.train(\n",
        "    total_timesteps=1000,\n",
        "    eval_env=eval_env,\n",
        "    ref_point=np.array([0.0, 0.0]),\n",
        "    eval_freq=100,\n",
        ")\n",
        "\n",
        "env_returns = np.array(envelope_agent.archive[\"vectors\"])\n",
        "env_metrics = compute_metrics(\"Envelope\", env_returns)\n",
        "plot_front(env_returns, \"Envelope Pareto Front\")\n",
        "\n",
        "\n",
        "fmdq_agent = FMDQ(\n",
        "    env,\n",
        "    log=True,\n",
        "    project_name=\"FMDQ-Run\"\n",
        ")\n",
        "\n",
        "fmdq_agent.train(\n",
        "    total_timesteps=5000,\n",
        "    eval_env=eval_env,\n",
        "    ref_point=np.array([0.0, 0.0]),\n",
        "    eval_freq=100,\n",
        ")\n",
        "\n",
        "fmdq_returns = np.array(fmdq_agent.archive[\"vectors\"])\n",
        "\n",
        "fmdq_metrics = compute_metrics(\"FMDQ\", fmdq_returns)\n",
        "\n",
        "plot_front(fmdq_returns, \"F-MDQ Pareto Front\")\n"
      ],
      "metadata": {
        "id": "lNn7fJxpS-tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n========== FINAL COMPARISON ==========\")\n",
        "print(f\"Envelope HV: {env_metrics['hv']:.4f} vs FMDQ HV: {fmdq_metrics['hv']:.4f}\")\n",
        "print(f\"Envelope CD: {env_metrics['cd']} vs FMDQ CD: {fmdq_metrics['cd']}\")\n",
        "print(f\"Envelope CV: {env_metrics['cv']:.4f} vs FMDQ CV: {fmdq_metrics['cv']:.4f}\")\n",
        "print(f\"Envelope GGF mean: {env_metrics['ggf_mean']:.4f} vs FMDQ GGF mean: {fmdq_metrics['ggf_mean']:.4f}\")"
      ],
      "metadata": {
        "id": "-5i7RExkTAZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}